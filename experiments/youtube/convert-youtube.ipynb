{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Youtube\n",
    "\n",
    "## Load and transform the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source:\n",
    "# =======\n",
    "# https://www.kaggle.com/datasnaek/youtube-new\n",
    "\n",
    "import io, os, sys\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.feature_extraction.text as fetext\n",
    "import qgrid\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "workdir = \"/tmp/experiments/youtube\"\n",
    "source_data_h5 = \"youtube.h5\"\n",
    "source_data_h5_path = os.path.join(workdir, source_data_h5)\n",
    "h5_key = \"dataset\"\n",
    "\n",
    "bitboost_path = \"..\"\n",
    "\n",
    "if not os.path.isdir(workdir):\n",
    "    os.makedirs(workdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prep the source data file if prepped files do not exist\n",
    "if not os.path.isfile(source_data_h5_path):\n",
    "    df1 = pd.read_csv(os.path.join(workdir, \"CAvideos.csv\"))\n",
    "    df2 = pd.read_csv(os.path.join(workdir, \"GBvideos.csv\"))\n",
    "    df3 = pd.read_csv(os.path.join(workdir, \"USvideos.csv\"))\n",
    "    df1[\"country\"] = 0\n",
    "    df2[\"country\"] = 1\n",
    "    df3[\"country\"] = 2\n",
    "    df = pd.concat([df1, df2, df3], axis=0, ignore_index=True)\n",
    "    \n",
    "    display(df1.shape)\n",
    "    display(df2.shape)\n",
    "    display(df3.shape)\n",
    "    display(df.shape)\n",
    "    \n",
    "    df.drop(columns=['video_id', 'thumbnail_link'], inplace=True)\n",
    "    display(df.columns)\n",
    "\n",
    "    print(\"[ ] cleaning tags and description\")\n",
    "    re_tag1 = re.compile('(\\||/)')\n",
    "    re_tag2 = re.compile('(\"|\\'|\\[none\\])')\n",
    "    df['tags'] = df['tags'].apply(lambda s: re_tag2.sub('', re_tag1.sub(' ', s)))\n",
    "    df['description'] = df['description'].apply(lambda s: s if isinstance(s, str) else '' )\n",
    "    \n",
    "    print(\"[ ] generating word corpus\")\n",
    "    corpus1 = list(df['title'])\n",
    "    corpus2 = list(df['channel_title'])\n",
    "    corpus3 = list(df['tags'])\n",
    "\n",
    "    corpus = list(map(lambda t: t[0] + ' ' + t[1] + ' ' + t[2],\n",
    "                      zip(corpus1, corpus2, corpus3)))\n",
    "    \n",
    "    print(\"[ ] generating term-document matrix\")\n",
    "    vectorizer = fetext.CountVectorizer(strip_accents='unicode', min_df=0.01, max_df=0.9, ngram_range=(1,4),\n",
    "                                        binary=True, lowercase=True)\n",
    "    tdm = vectorizer.fit_transform(corpus)\n",
    "    tdm_dense = tdm.todense().astype(np.uint8)\n",
    "    \n",
    "    print(\"[ ] generating nÂ° word features\")\n",
    "    features = {}\n",
    "    re_space = re.compile('\\s+')\n",
    "\n",
    "    features['title_nchars'] = list(map(len, df['title']))\n",
    "    features['title_nwords'] = list(map(lambda x: len(re_space.split(x)), df['title']))\n",
    "    features['channel_nchars'] = list(map(len, df['channel_title']))\n",
    "    features['channel_nwords'] = list(map(lambda x: len(re_space.split(x)), df['channel_title']))\n",
    "    features['descr_nchars'] = list(map(len, df['description']))\n",
    "    features['descr_nwords'] = list(map(lambda x: len(re_space.split(x)), df['description']))\n",
    "    features['ntags'] = list(map(lambda x: len(re_space.split(x)), df['tags']))\n",
    "    \n",
    "    print(\"[ ] generating trend date features\")\n",
    "    features['trend_year'] = list(map(lambda x: datetime.strptime(x, '%y.%d.%m').year, df['trending_date']))\n",
    "    features['trend_month'] = list(map(lambda x: datetime.strptime(x, '%y.%d.%m').month, df['trending_date']))\n",
    "    features['trend_day'] = list(map(lambda x: datetime.strptime(x, '%y.%d.%m').day, df['trending_date']))\n",
    "    features['trend_wday'] = list(map(lambda x: datetime.strptime(x, '%y.%d.%m').weekday(), df['trending_date']))\n",
    "    \n",
    "    print(\"[ ] generating publish date features\")\n",
    "    features['publish_year'] = list(map(lambda x: datetime.strptime(x[0:13], '%Y-%m-%dT%H').year, df['publish_time']))\n",
    "    features['publish_month'] = list(map(lambda x: datetime.strptime(x[0:13], '%Y-%m-%dT%H').month, df['publish_time']))\n",
    "    features['publish_day'] = list(map(lambda x: datetime.strptime(x[0:13], '%Y-%m-%dT%H').day, df['publish_time']))\n",
    "    features['publish_hour'] = list(map(lambda x: datetime.strptime(x[0:13], '%Y-%m-%dT%H').hour, df['publish_time']))\n",
    "    features['publish_wday'] = list(map(lambda x: datetime.strptime(x[0:13], '%Y-%m-%dT%H').weekday(), df['publish_time']))\n",
    "        \n",
    "    print(\"[ ] generating transformed features\")\n",
    "    features['dislikeslg'] = np.log(df['dislikes'] + 1, dtype=np.float32)\n",
    "    features['likeslg'] = np.log(df['likes'] + 1, dtype=np.float32)\n",
    "    features['dislikeratiolg'] = np.log((df['dislikes'] + 1) / (df['likes'] + df['dislikes'] + 1), dtype=np.float32)\n",
    "    features['cmtslg'] = np.log(df['comment_count'] + 1, dtype=np.float32)\n",
    "    features['likepcmtlg'] = np.log((df['comment_count'] + 1) / (df['likes'] + df['dislikes'] + 1), dtype=np.float32)\n",
    "    #features[\"viewslg\"] = np.log(df['views'] + 1, dtype=np.float32)\n",
    "   \n",
    "    print(\"[ ] generating categorical features\")\n",
    "    cat_features = {}\n",
    "    cat_features['cat_id'] = list(df['category_id'])\n",
    "    cat_features['country'] = list(df['country'])\n",
    "    cat_features['cmtsdis'] = list(map(lambda x: 1 if x else 0, df['comments_disabled']))\n",
    "    cat_features['likedis'] = list(map(lambda x: 1 if x else 0, df['ratings_disabled']))\n",
    "    cat_features['err'] = list(map(lambda x: 1 if x else 0, df['video_error_or_removed']))\n",
    "\n",
    "    print(\"[ ] generating target\")\n",
    "    #features['target'] = (df[\"likes\"] > df[\"dislikes\"]).astype(np.uint8)\n",
    "    #features['target'] = np.log(((df[\"likes\"]+1) / (df[\"dislikes\"]+1)), dtype=np.float32)\n",
    "    features[\"viewslg\"] = np.log10(df['views'] + 1, dtype=np.float32)\n",
    "    \n",
    "    print(\"[ ] combining dataframe\")\n",
    "    n = df.shape[0]\n",
    "    colnames = list(map(lambda x: \"txt_\"+re_space.sub('_', x), vectorizer.get_feature_names()))\n",
    "    df1 = pd.DataFrame(tdm_dense, columns=colnames, index=range(n), dtype=np.uint32)\n",
    "    df2 = pd.DataFrame(features, index=range(n), dtype=np.float32)\n",
    "    df3 = pd.DataFrame(cat_features, index=range(n), dtype=np.uint32)\n",
    "    df_comb  = pd.concat([df1, df3, df2], axis=1)\n",
    "    display(df1.shape)\n",
    "    display(df2.shape)\n",
    "    display(df3.shape)\n",
    "    display(df_comb.shape)\n",
    "\n",
    "    print(\"[ ] write to hdf5\")\n",
    "    df_comb.to_hdf(source_data_h5_path, h5_key, complevel=9)\n",
    "    \n",
    "    print(\"[ ] done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
